---
title: "ANALISI DATASET depression_heart_failure"
output: html_notebook
---
ANALISI DATASET depression_heart_failure.csv

```{r}
library(readr)
library(fpc)
library(cluster)

source("~/Calinski_Harabasz_Thesis/Script/FunzioniDatasetReali.R")

# Lettura e pulizia del dataset da dati NA
dataset <- na.omit(read.csv("~/Calinski_Harabasz_Thesis/Data/DatasetReali/depression_heart_failure.csv"))

# Creazione dataframe per i risultati
risultati <- data.frame(matrix(ncol = 9, nrow = 0))

colnames(risultati) <- c("metodo" , "k" , "distance" , "nstart" , "eps" , "minPts" , "k_out" , "linkage" , "CH")

```

Algoritmo di clustering: K-means

Iperparametri:

-   k = (2, 3, 5)

-   distance = ("euclidean", "manhattan")

-   nstart = (2, 20, 100)

```{r}

# Creazione lista con i valori di k
list_k <- c(2,3,5)

# Creazione lista con valori di distance
list_distance <- c("euclidean", "manhattan")

# Creazione lista con valori di nstart
list_nstart <- c(2,20,100)
 


for (i in 1:length(list_k)) {
  for (j in 1:length(list_distance)){
    for (n in 1:length(list_nstart)) {
      
      # Calcolo della matrice di distanza dei dati
      diss <- dist(dataset, method = list_distance[j])
      
      # Clustering con K-means
      obj <- pam(diss, list_k[i], nstart = list_nstart[n])
      
      # Validazione con Calinski-Harabasz tramite calinhara
      valore <- calinhara(dataset, obj$cluster)
      
      # Creazione riga della tabella risultati 
      new_row <- data.frame(metodo = "K-means", k = list_k[i], distance = list_distance[j], nstart = list_nstart[n], eps = NA, minPts = NA, k_out = NA, linkage = NA, CH = valore)
      
      # Aggiunta della riga al dataframe risultati
      risultati <- rbind(risultati, new_row)
    }
  }
}


print(risultati)

```

Quindi verifichiamo che applicando l'algoritmo di clustering K-means, la metrica di Calinski-Harabasz individua come miglior configurazione quella con il valore di k=5, distance="euclidean" ed nstart indifferente rispetto a quelle scelte.



------------------------------------------------------------------------

Algoritmo di clustering: DBSCAN

Iperparametri:

-   ðœ€ = (80, 88, 96) 

-   minPts = (14, 15, 16)

```{r}

# Creazione lista con i valori di eps
list_eps <- c(80, 88, 96)

# Creazione lista con i valori di minPts
list_minPts <- c(14, 15, 16)




# Ciclo per la validazione del clustering
for (i in 1:length(list_eps)) { 
  for (j in 1:length(list_minPts)) {
    
    # Clustering con DBSCAN 
    clustering <- dbscan(dataset, list_eps[i], list_minPts[j])
    
    # Validazione con Calinski-Harabasz tramite calinhara
    valore <- calinhara(dataset, clustering$cluster)
    
    # Creazione riga della tabella risultati 
      new_row <- data.frame(metodo = "DBSCAN", k = NA, distance = NA, nstart = NA, eps = list_eps[i], minPts = list_minPts[j], k_out = max(clustering$cluster), linkage = NA, CH = valore)
      
      # Aggiunta della riga al dataframe risultati
      risultati <- rbind(risultati, new_row)
  }
}



# Stampa dataframe 
print(risultati)

```

Quindi verifichiamo che applicando l'algoritmo di clustering DBSCAN, la metrica di Calinski-Harabasz individua come miglior configurazione quella con i valori di ðœ€=88, minPts=15 e k_out=2 rispetto a quelle scelte.

------------------------------------------------------------------------

Algoritmo di clustering: Hierarchical clustering

Iperparametri:

-   linkage = ("complete", "average", "single")

-   distance = ("euclidean", "manhattan")


```{r}

# Creazione lista con i valori di method
list_linkage <- c("complete", "average", "single")

# Creazione dataframe per i risultati
risultati.hier <- data.frame(k = 2:10)

# Ciclo per clustering tramite hierarchical clustering e validazione con calinhara
for(i in list_linkage){
  for (j in list_distance) {
    
    # Creazione lista per i valori di Calinski-Harabasz 
    CH.hier<- c()
    
    # Clustering con complete-linkage
    H.model <- hclust(dist(dataset, method = j), i)
    
    # Ciclo per la validazione del clustering con calinhara
    for (x in 2:10) {
      
      #Taglio dell'albero risultante dal clustering
      cluster <- cutree(H.model, k = x) 
      
      #Aggiornamento lista coi risultati
      CH.hier <- c(CH.hier, calinhara(dataset, cluster))  
    }
    
    # Aggiornamento dataframe risultati
    if(i == "complete"){
      
      # Aggiornamento dataframe Hierarchical-clustering
      risultati.hier$complete <- CH.hier 
      
      # Calcolo ottimo locale per complete-link
      cmplt <- localOpt(risultati.hier$complete)    
      
      # Selezione riga con ottimo locale
      row_sel <- risultati.hier[risultati.hier$complete==cmplt , ]
      
      # Creazione riga della tabella risultati 
        new_row <- data.frame(metodo = "Hierarchical-clustering", k = row_sel$k, distance = j, nstart = NA, eps = NA, minPts = NA, k_out = NA, linkage = "complete", CH = row_sel$complete)
        
        # Aggiunta della riga al dataframe risultati
        risultati <- rbind(risultati, new_row)
    }
    
    if(i == "average"){
      
      # Aggiornamento dataframe Hierarchical-clustering
      risultati.hier$average <- CH.hier 
      
      # Calcolo ottimo locale per average-link
      avrg <- localOpt(risultati.hier$average)    
      
      # Selezione riga con ottimo locale
      row_sel <- risultati.hier[risultati.hier$average==avrg , ]
      
      # Creazione riga della tabella risultati 
        new_row <- data.frame(metodo = "Hierarchical-clustering", k = row_sel$k, distance = j, nstart = NA, eps = NA, minPts = NA, k_out = NA, linkage = "average", CH = row_sel$average)
        
        # Aggiunta della riga al dataframe risultati
        risultati <- rbind(risultati, new_row)
    }
    
    if(i == "single"){
      
      # Aggiornamento dataframe Hierarchical-clustering
      risultati.hier$single <- CH.hier 
      
      # Calcolo ottimo locale per single-link
      sngl <- localOpt(risultati.hier$single) 
      
      # Selezione riga con ottimo locale
      row_sel <- risultati.hier[risultati.hier$single==sngl , ]
      
      # Creazione riga della tabella risultati 
        new_row <- data.frame(metodo = "Hierarchical-clustering", k = row_sel$k, distance = j, nstart = NA, eps = NA, minPts = NA, k_out = NA, linkage = "single", CH = row_sel$single)
        
        # Aggiunta della riga al dataframe risultati
        risultati <- rbind(risultati, new_row)
    }
  }
}

# Stampa del dataframe dei risultati
print(risultati)

```

Per scegliere la configurazione migliore ci limitiamo ad analizzare il clustering per le configurazioni che vanno da 2 a 10 cluster cercando un ottimo locale con il valore di k minimo.
Per l'algoritmo di clustering Hierarchical clustering, la metrica di Calinski-Harabasz individua come miglior configurazione quella con tipo average-linkage e distance=euclidean rispetto alle altre opzioni, con ottimo locale per k=3.

CONCLUSIONI:
```{r}

# Stampa miglior valore di Calinski-Harabasz per le configurazioni analizzate 
cat(sprintf("\nValore di Calinski-Harabasz migliore per le configurazioni analizzate : %f\n",max(risultati$CH)))

# Stampa della configurazione con il valore trovato
(risultati[risultati$CH==max(risultati$CH), ])

```

L'analisi effettuata tramite la metrica di Calinski-Harabasz indica come miglior risultato la combinazione di DBSCAN come algoritmo di clustering con configurazione di iperparametri ðœ€=88 e minPts=15.
Notiamo inoltre come per ogni algoritmo il numero di cluster ottimale risulti diverso; per K-means(k=5), per DBSCAN(k=4) mentre per Hierarchical-clustering(k=3).