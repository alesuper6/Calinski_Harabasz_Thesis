---
title: "ANALISI DATASET diabetes_type1"
output: html_notebook
---

ANALISI DATASET diabetes_type1.csv

Algoritmo di clustering: K-means

Iperparametri:

-   k=2 , distance="euclidean" , nstart=2

-   k=3 , distance="manhattan" , nstart=20

-   k=5 , distance="euclidean" , nstart=100
 
```{r}
library(readr)
library(fpc)
library(cluster)

source("~/Calinski_Harabasz_Thesis/Script/FunzioniDatasetReali.R")

# Lettura e pulizia del dataset da dati NA
dataset <- na.omit(read.csv("~/Calinski_Harabasz_Thesis/Data/DatasetReali/diabetes_type1.csv"))

# Creazione lista con i valori di k
k <- c(2,3,5)

# Creazione lista con valori di method
method <- c("euclidean", "manhattan", "euclidean")

# Creazione lista con valori di nstart
nstart <- c(2,20,100)
 
# Creazione lista per i valori di Calinski-Harabasz
CH.kmeans <- c()

for (i in 1:length(k)) {
  
  # Calcolo della matrice di distanza dei dati
  diss <- dist(dataset, method = method[i])
  
  # Clustering con K-means
  obj <- pam(diss, k[i], nstart = nstart[i])
  
  # Validazione con Calinski-Harabasz tramite calinhara
  valore <- calinhara(dataset, obj$cluster)
  
  # Aggiungiamo il valore alla lista
  CH.kmeans <- c(CH.kmeans, valore)
  
}

# Creazione dataframe /valore
risultati.kmeans <- data.frame(k = k, distance = method, nstart = nstart, val.CH = CH.kmeans)

print(risultati.kmeans)

# Calcolo del valore migliore di Calinski-Harabasz
kmeansOpt <- max(risultati.kmeans$val.CH)
cat(sprintf("\nValore di Calinski-Harabasz migliore per K-means : %f\n",kmeansOpt))

```

Quindi verifichiamo che applicando l'algoritmo di clustering K-means, la metrica di Calinski-Harabasz individua come miglior configurazione quella con il valore di k=2 rispetto a quelle scelte.



------------------------------------------------------------------------

Algoritmo di clustering: DBSCAN

Iperparametri:

-   ðœ€=20 , minPts=5 

-   ðœ€=22 , minPts=5 

-   ðœ€=20 , minPts=6 

```{r}

# Creazione lista con i valori di eps
list_eps <- c(20, 22, 20)

# Creazione lista con i valori di minPts
list_minPts <- c(5, 5, 6)

# Creazione lista per i valori di Calinski-Harabasz
CH.dbscan <- c()

# Controllo lunghezza liste
if(length(list_eps)==length(list_minPts)){
  
  # Ciclo per la validazione del clustering
  for (i in 1:length(list_eps)) { 
    eps <- list_eps[i]          #estrae il valore di eps
    minPts <- list_minPts[i]    #estrae il valore di minPts
    
    # Clustering con DBSCAN 
    clustering <- dbscan(dataset, eps, minPts)
    
    # Aggiornamento lista coi risultati
    CH.dbscan <- c(CH.dbscan, calinhara(dataset, clustering$cluster))
  }
  
  # Creazione dataframe iterazione\valore
  risultati.dbscan <- data.frame(eps = list_eps, minPts = list_minPts, val.CH = CH.dbscan)
  
  # Stampa dataframe 
  print(risultati.dbscan)
  
} else {
  print("Le liste devono avere la stessa lunghezza")  
}

# Calcolo del valore migliore di Calinski-Harabasz
dbscanOpt <- max(risultati.dbscan$val.CH)
cat(sprintf("\nValore di Calinski-Harabasz migliore per DBSCAN : %f\n",dbscanOpt))

```

Quindi verifichiamo che applicando l'algoritmo di clustering DBSCAN, la metrica di Calinski-Harabasz individua come miglior configurazione quella con i valori di ðœ€=20 e minPts= 5 rispetto a quelle scelte.
Le due configurazioni che raggiungono il valore di Calinski-Harabasz infinito sono meglio dal punto di vista della metrica ma poco indicative in un contesto di confronto.

------------------------------------------------------------------------

Algoritmo di clustering: Hierarchical clustering

Iperparametri:

-   complete-linkage

-   average-linkage

-   single-linkage


```{r}

# Creazione lista con i valori di method
method <- c("complete", "average", "single")

# Creazione dataframe per i risultati
risultati.hier <- data.frame(k = 2:10)

# Ciclo per clustering tramite hierarchical clustering e validazione con calinhara
for(i in method){
  
  # Creazione lista per i valori di Calinski-Harabasz 
  CH.hier<- c()
  
  # Clustering con complete-linkage
  H.model <- hclust(dist(dataset), i)
  
  # Ciclo per la validazione del clustering con calinhara
  for (j in 2:10) {
    cluster <- cutree(H.model, k = j)  #Taglio dell'albero risultante dal clustering
    CH.hier <- c(CH.hier, calinhara(dataset, cluster))  #Aggiornamento lista coi risultati
  }
  
  # Aggiornamento dataframe risultati
  if(i == "complete"){
    risultati.hier$complete <- CH.hier 
  }
  if(i == "average"){
    risultati.hier$average <- CH.hier 
  }
  if(i == "single"){
    risultati.hier$single <- CH.hier 
  }
}

# Stampa del dataframe dei risultati
print(risultati.hier)

# Calcolo del valore migliore di Calinski-Harabasz

cmplt <- localOpt(risultati.hier$complete)    #Calcolo ottimo locale per complete-link
avrg <- localOpt(risultati.hier$average)      #Calcolo ottimo locale per average-link
sngl <- localOpt(risultati.hier$single)       #Calcolo ottimo locale per single-link
hclustOpt <- max(c(cmplt,avrg,sngl))
cat(sprintf("\nValore di Calinski-Harabasz migliore per DBSCAN : %f\n",hclustOpt))

```

Per scegliere la configurazione migliore ci limitiamo ad analizzare il clustering per le configurazioni che vanno da 2 a 10 cluster cercando un ottimo locale con il valore di k minimo.
Per l'algoritmo di clustering Hierarchical clustering, la metrica di Calinski-Harabasz individua come miglior configurazione quella con tipo average-linkage rispetto alle altre opzioni con ottimo locale per k=2.

CONCLUSIONI:
```{r}
cat(sprintf("\nValore di Calinski-Harabasz migliore per le configurazioni analizzate : %f\n",max(c(kmeansOpt,dbscanOpt,hclustOpt))))
```
L'analisi effettuata tramite la metrica di Calinski-Harabasz indica come miglior risultato la combinazione di DBSCAN come algoritmo di clustering con configurazione di iperparametri ðœ€=20 e minPts= 5.


