---
title: "ANALISI DATASET diabetes_type1"
output: html_notebook
---

ANALISI DATASET diabetes_type1.csv

```{r}
library(readr)
library(fpc)
library(cluster)

source("~/Calinski_Harabasz_Thesis/Script/FunzioniDatasetReali.R")

# Lettura e pulizia del dataset da dati NA
dataset <- na.omit(read.csv("~/Calinski_Harabasz_Thesis/Data/DatasetReali/diabetes_type1.csv"))

# Creazione dataframe per i risultati
risultati <- data.frame(matrix(ncol = 8, nrow = 0))

colnames(risultati) <- c("metodo" , "k" , "distance" , "nstart" , "eps" , "minPts" , "linkage" , "CH")

```

Algoritmo di clustering: K-means

Iperparametri:

-   k = (2, 3, 5)

-   distance = ("euclidean", "manhattan")

-   nstart = (2, 20, 100)
 
```{r}
# Creazione lista con i valori di k
list_k <- c(2,3,5)

# Creazione lista con valori di distance
list_distance <- c("euclidean", "manhattan")

# Creazione lista con valori di nstart
list_nstart <- c(2,20,100)
 


for (i in 1:length(list_k)) {
  for (j in 1:length(list_distance)){
    for (n in 1:length(list_nstart)) {
      
      # Calcolo della matrice di distanza dei dati
      diss <- dist(dataset, method = list_distance[j])
      
      # Clustering con K-means
      obj <- pam(diss, list_k[i], nstart = list_nstart[n])
      
      # Validazione con Calinski-Harabasz tramite calinhara
      valore <- calinhara(dataset, obj$cluster)
      
      # Creazione riga della tabella risultati 
      new_row <- data.frame(metodo = "K-means", k = list_k[i], distance = list_distance[j], nstart = list_nstart[n], eps = NA, minPts = NA, linkage = NA, CH = valore)
      
      # Aggiunta della riga al dataframe risultati
      risultati <- rbind(risultati, new_row)
    }
  }
}


print(risultati)

```

Quindi verifichiamo che applicando l'algoritmo di clustering K-means, la metrica di Calinski-Harabasz individua come miglior configurazione quella con il valore di k=2, distance="euclidean" e nstart indifferente rispetto a quelle scelte.



------------------------------------------------------------------------

Algoritmo di clustering: DBSCAN

Iperparametri:

-   𝜀 = (20) 

-   minPts = (2, 5)
```{r}

# Creazione lista con i valori di eps
list_eps <- c(20)

# Creazione lista con i valori di minPts
list_minPts <- c(2, 5)


# Ciclo per la validazione del clustering
for (i in 1:length(list_eps)) { 
  for (j in 1:length(list_minPts)) {
    
    # Clustering con DBSCAN 
    clustering <- dbscan(dataset, list_eps[i], list_minPts[j])
    
    # Validazione con Calinski-Harabasz tramite calinhara
    valore <- calinhara(dataset, clustering$cluster)
    
    # Creazione riga della tabella risultati 
      new_row <- data.frame(metodo = "DBSCAN", k = NA, distance = NA, nstart = NA, eps = list_eps[i], minPts = list_minPts[j], linkage = NA, CH = valore)
      
      # Aggiunta della riga al dataframe risultati
      risultati <- rbind(risultati, new_row)
  }
}



# Stampa dataframe 
print(risultati)

```

Quindi verifichiamo che applicando l'algoritmo di clustering DBSCAN, la metrica di Calinski-Harabasz individua come miglior configurazione quella con i valori di 𝜀=20 e minPts= 5 rispetto a quelle scelte.
Nello studio di questo dataset ci limitiamo a poche configurazioni poichè l'indice restituisce valori confrontabili solo per poche di esse.

------------------------------------------------------------------------

Algoritmo di clustering: Hierarchical clustering

Iperparametri:

-   linkage = ("complete", "average", "single")


```{r}


# Creazione lista con i valori di method
list_linkage <- c("complete", "average", "single")

# Creazione dataframe per i risultati
risultati.hier <- data.frame(k = 2:10)

# Ciclo per clustering tramite hierarchical clustering e validazione con calinhara
for(i in list_linkage){
  
  # Creazione lista per i valori di Calinski-Harabasz 
  CH.hier<- c()
  
  # Clustering con complete-linkage
  H.model <- hclust(dist(dataset), i)
  
  # Ciclo per la validazione del clustering con calinhara
  for (j in 2:10) {
    
    #Taglio dell'albero risultante dal clustering
    cluster <- cutree(H.model, k = j) 
    
    #Aggiornamento lista coi risultati
    CH.hier <- c(CH.hier, calinhara(dataset, cluster))  
  }
  
  # Aggiornamento dataframe risultati
  if(i == "complete"){
    
    # Aggiornamento dataframe Hierarchical-clustering
    risultati.hier$complete <- CH.hier 
    
    # Calcolo ottimo locale per complete-link
    cmplt <- localOpt(risultati.hier$complete)    
    
    # Selezione riga con ottimo locale
    row_sel <- risultati.hier[risultati.hier$complete==cmplt , ]
    
    # Creazione riga della tabella risultati 
      new_row <- data.frame(metodo = "Hierarchical-clustering", k = row_sel$k, distance = NA, nstart = NA, eps = NA, minPts = NA, linkage = "complete", CH = row_sel$complete)
      
      # Aggiunta della riga al dataframe risultati
      risultati <- rbind(risultati, new_row)
  }
  
  if(i == "average"){
    
    # Aggiornamento dataframe Hierarchical-clustering
    risultati.hier$average <- CH.hier 
    
    # Calcolo ottimo locale per average-link
    avrg <- localOpt(risultati.hier$average)    
    
    # Selezione riga con ottimo locale
    row_sel <- risultati.hier[risultati.hier$average==avrg , ]
    
    # Creazione riga della tabella risultati 
      new_row <- data.frame(metodo = "Hierarchical-clustering", k = row_sel$k, distance = NA, nstart = NA, eps = NA, minPts = NA, linkage = "average", CH = row_sel$average)
      
      # Aggiunta della riga al dataframe risultati
      risultati <- rbind(risultati, new_row)
  }
  
  if(i == "single"){
    
    # Aggiornamento dataframe Hierarchical-clustering
    risultati.hier$single <- CH.hier 
    
    # Calcolo ottimo locale per single-link
    sngl <- localOpt(risultati.hier$single) 
    
    # Selezione riga con ottimo locale
    row_sel <- risultati.hier[risultati.hier$single==sngl , ]
    
    # Creazione riga della tabella risultati 
      new_row <- data.frame(metodo = "Hierarchical-clustering", k = row_sel$k, distance = NA, nstart = NA, eps = NA, minPts = NA, linkage = "single", CH = row_sel$single)
      
      # Aggiunta della riga al dataframe risultati
      risultati <- rbind(risultati, new_row)
  }
}

# Stampa del dataframe dei risultati
print(risultati)

```

Per scegliere la configurazione migliore ci limitiamo ad analizzare il clustering per le configurazioni che vanno da 2 a 10 cluster cercando un ottimo locale con il valore di k minimo.
Per l'algoritmo di clustering Hierarchical clustering, la metrica di Calinski-Harabasz individua come miglior configurazione quella con tipo average-linkage rispetto alle altre opzioni con ottimo locale per k=2.

CONCLUSIONI:
```{r}

# Stampa miglior valore di Calinski-Harabasz per le configurazioni analizzate 
cat(sprintf("\nValore di Calinski-Harabasz migliore per le configurazioni analizzate : %f\n",max(risultati$CH)))

# Stampa della configurazione con il valore trovato
(risultati[risultati$CH==max(risultati$CH), ])

```
L'analisi effettuata tramite la metrica di Calinski-Harabasz indica come miglior risultato la combinazione di DBSCAN come algoritmo di clustering con configurazione di iperparametri 𝜀=20 e minPts= 5.


